{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\Adaku\\\\Documents\\\\Authorship-Attribution-for-Neural-Text-Generation\\\\data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + 'all_with_punctuation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>latest headlines on cnn business tl;dr the u.s...</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>china wants to take a victory lap over its han...</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disinformation creates challenges ...</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>china coronavirus: eating wild animals made il...</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china's economy could shrink for the first tim...</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  latest headlines on cnn business tl;dr the u.s...  ctrl\n",
       "1  china wants to take a victory lap over its han...  ctrl\n",
       "2  coronavirus disinformation creates challenges ...  ctrl\n",
       "3  china coronavirus: eating wild animals made il...  ctrl\n",
       "4  china's economy could shrink for the first tim...  ctrl"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adaku\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, label):\n",
    "            \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=1234,n_estimators=150,n_jobs=-1)\n",
    "#     clf = LogisticRegression(solver = 'lbfgs',multi_class='auto')\n",
    "    \n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "#     train_corpus = [[word.lower() for word in text.split()] for text in data]\n",
    "\n",
    "    test_corpus = X_test\n",
    "#     test_corpus = [\" \".join(x) for x in test_corpus]\n",
    "\n",
    "#     ax = axes.set_ylim([0,300])\n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "#     # Instantiate the classification model and visualizer\n",
    "#     visualizer = ClassPredictionError(clf, classes= ['human', 'ctrl', 'gpt', 'gpt2', 'grover', 'xlm', 'xlnet', 'pplm', 'fair'])\n",
    "\n",
    "#     # Fit the training data to the visualizer\n",
    "#     visualizer.fit(train_vector, y_train)\n",
    "\n",
    "#     # Evaluate the model on the test data\n",
    "#     visualizer.score(test_vector, y_test)\n",
    "\n",
    "#     # Draw visualization\n",
    "#     visualizer.show()\n",
    "    \n",
    "    \n",
    "    matrix = confusion_matrix(y_test, pred, labels = ['ctrl', 'gpt', 'gpt2', 'grover', 'xlm', 'xlnet', 'pplm', 'human', 'fair'])\n",
    "    mat = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    print(classification_report(y_test, pred, labels = ['ctrl', 'gpt', 'gpt2', 'grover','xlm', 'xlnet', 'pplm', 'human', 'fair'],\n",
    "                                digits=4))\n",
    "#     print('confusion matrix: ', mat)\n",
    "    \n",
    "#     Accuracy = accuracy_score(y_test,pred)\n",
    "#     F1 = f1_score(y_test, pred, average='macro')\n",
    "#     print(\"Accuracy:\", Accuracy)\n",
    "    \n",
    "#     rec = recall_score(y_test, pred, average='macro')\n",
    "#     print('Recall: ', rec)\n",
    "#     prec = precision_score(y_test, pred, average='macro')\n",
    "#     print('Precision: ', prec)\n",
    "    \n",
    "#     print('F1:', F1)\n",
    "    \n",
    "    return clf, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl     0.9813    0.9859    0.9836       213\n",
      "         gpt     0.9725    0.9953    0.9838       213\n",
      "        gpt2     0.6000    0.7710    0.6748       214\n",
      "      grover     0.7366    0.7089    0.7225       213\n",
      "         xlm     0.9953    0.9859    0.9906       213\n",
      "       xlnet     0.9953    0.9859    0.9906       213\n",
      "        pplm     0.8427    0.7009    0.7653       214\n",
      "       human     0.9422    0.7653    0.8446       213\n",
      "        fair     0.5769    0.6338    0.6040       213\n",
      "\n",
      "    accuracy                         0.8369      1919\n",
      "   macro avg     0.8492    0.8370    0.8400      1919\n",
      "weighted avg     0.8491    0.8369    0.8398      1919\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                        n_jobs=-1, oob_score=False, random_state=1234, verbose=0,\n",
       "                        warm_start=False),\n",
       " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(data['text'], data['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(data):\n",
    "    pos = []\n",
    "    for i in data:\n",
    "        pos.append( nltk.pos_tag(word_tokenize(i)))\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = get_pos(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9594"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('china', 'NN'),\n",
       " ('wants', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('victory', 'NN'),\n",
       " ('lap', 'NN'),\n",
       " ('over', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('handling', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('coronavirus', 'NN'),\n",
       " ('outbreak', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('hong', 'JJ'),\n",
       " ('kong', 'NN'),\n",
       " ('.', '.'),\n",
       " ('the', 'DT'),\n",
       " ('virus', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('killed', 'VBN'),\n",
       " ('at', 'IN'),\n",
       " ('least', 'JJS'),\n",
       " ('21', 'CD'),\n",
       " ('people', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('infected', 'VBD'),\n",
       " ('more', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('1,000', 'CD'),\n",
       " ('since', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('first', 'RB'),\n",
       " ('detected', 'VBN'),\n",
       " ('there', 'RB'),\n",
       " ('last', 'JJ'),\n",
       " ('month', 'NN'),\n",
       " ('.', '.'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('believed', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('most', 'JJS'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('chinese', 'JJ'),\n",
       " ('citizens', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('had', 'VBD'),\n",
       " ('traveled', 'VBN'),\n",
       " ('abroad', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('treatment', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('were', 'VBD'),\n",
       " ('working', 'VBG'),\n",
       " ('as', 'IN'),\n",
       " ('doctors', 'NNS'),\n",
       " ('there', 'RB'),\n",
       " ('.', '.'),\n",
       " ('but', 'CC'),\n",
       " ('health', 'NN'),\n",
       " ('officials', 'NNS'),\n",
       " ('say', 'VBP'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('still', 'RB'),\n",
       " ('investigating', 'VBG'),\n",
       " ('whether', 'IN'),\n",
       " ('anyone', 'NN'),\n",
       " ('else', 'RB'),\n",
       " ('may', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('infected', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('a', 'DT'),\n",
       " ('total', 'JJ'),\n",
       " ('ban', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('travel', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('remains', 'VBZ'),\n",
       " ('until', 'IN'),\n",
       " ('further', 'JJ'),\n",
       " ('notice', 'NN'),\n",
       " ('.', '.'),\n",
       " ('[', 'JJ'),\n",
       " ('page', 'NN'),\n",
       " ('a8', 'NN'),\n",
       " ('.', '.'),\n",
       " (']', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('addition', 'NN'),\n",
       " ('today', 'NN'),\n",
       " (',', ','),\n",
       " ('china', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('ministry', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('health', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('tests', 'NNS'),\n",
       " ('showed', 'VBD'),\n",
       " ('evidence', 'NN'),\n",
       " ('suggesting', 'VBG'),\n",
       " ('an', 'DT'),\n",
       " ('unusual', 'JJ'),\n",
       " ('strain', 'NN'),\n",
       " ('called', 'VBN'),\n",
       " ('sars', 'NNS'),\n",
       " ('could', 'MD'),\n",
       " ('spread', 'VB'),\n",
       " ('through', 'IN'),\n",
       " ('humans', 'NNS'),\n",
       " ('if', 'IN'),\n",
       " ('not', 'RB'),\n",
       " ('properly', 'RB'),\n",
       " ('treated', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('[', 'JJ'),\n",
       " ('excerpts', 'NNS'),\n",
       " (',', ','),\n",
       " ('page', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('.', '.'),\n",
       " (']', 'CC'),\n",
       " ('dr.', 'JJ'),\n",
       " ('wu', 'NN'),\n",
       " ('jianmin', 'NN'),\n",
       " (',', ','),\n",
       " ('director', 'NN'),\n",
       " ('general', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('guangdong', 'JJ'),\n",
       " ('province', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('includes', 'VBZ'),\n",
       " ('guangzhou', 'NN'),\n",
       " (',', ','),\n",
       " ('told', 'VBD'),\n",
       " ('reporters', 'NNS'),\n",
       " ('today', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('expected', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('significant', 'JJ'),\n",
       " ('outbreak', 'NN'),\n",
       " ('within', 'IN'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('he', 'PRP'),\n",
       " ('added', 'VBD'),\n",
       " ('later', 'RB'),\n",
       " ('by', 'IN'),\n",
       " ('telephone', 'NN'),\n",
       " ('that', 'IN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " (\"'re\", 'VBP'),\n",
       " ('prepared', 'JJ'),\n",
       " ('should', 'MD'),\n",
       " ('something', 'NN'),\n",
       " ('happen', 'VB'),\n",
       " ('.', '.'),\n",
       " ('at', 'IN'),\n",
       " ('issue', 'NN'),\n",
       " ('here', 'RB'),\n",
       " ('will', 'MD'),\n",
       " ('likely', 'RB'),\n",
       " ('turn', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('either', 'DT'),\n",
       " ('one', 'CD'),\n",
       " ('way', 'NN'),\n",
       " ('--', ':'),\n",
       " ('another', 'DT'),\n",
       " ('case', 'NN'),\n",
       " ('confirms', 'VBZ'),\n",
       " ('what', 'WP'),\n",
       " ('many', 'JJ'),\n",
       " ('suspect', 'VBP'),\n",
       " ('but', 'CC'),\n",
       " ('no', 'DT'),\n",
       " ('conclusive', 'JJ'),\n",
       " ('proof', 'NN'),\n",
       " ('supports', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('either', 'DT'),\n",
       " ('way', 'NN'),\n",
       " (',', ','),\n",
       " ('beijing', 'VBG'),\n",
       " ('would', 'MD'),\n",
       " ('probably', 'RB'),\n",
       " ('try', 'VB'),\n",
       " ('hard', 'JJ'),\n",
       " ('just', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('before', 'IN'),\n",
       " ('rushing', 'VBG'),\n",
       " ('into', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('action', 'NN'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('while', 'IN'),\n",
       " ('some', 'DT'),\n",
       " ('experts', 'NNS'),\n",
       " ('believe', 'VBP'),\n",
       " ('such', 'JJ'),\n",
       " ('action', 'NN'),\n",
       " ('might', 'MD'),\n",
       " ('delay', 'VB'),\n",
       " ('even', 'RB'),\n",
       " ('worse', 'JJR'),\n",
       " ('problems', 'NNS'),\n",
       " (',', ','),\n",
       " ('others', 'NNS'),\n",
       " ('think', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('better', 'JJR'),\n",
       " ('late', 'JJ'),\n",
       " ('rather', 'RB'),\n",
       " ('then', 'RB'),\n",
       " ('never', 'RB'),\n",
       " ('.', '.'),\n",
       " ('even', 'RB'),\n",
       " ('so', 'RB'),\n",
       " (',', ','),\n",
       " ('both', 'DT'),\n",
       " ('sides', 'NNS'),\n",
       " ('seem', 'VBP'),\n",
       " ('intent', 'JJ'),\n",
       " ('upon', 'IN'),\n",
       " ('taking', 'VBG'),\n",
       " ('advantage', 'NN'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.'),\n",
       " ('with', 'IN'),\n",
       " ('so', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('attention', 'NN'),\n",
       " ('focused', 'VBD'),\n",
       " ('elsewhere', 'RB'),\n",
       " (',', ','),\n",
       " ('few', 'JJ'),\n",
       " ('outside', 'JJ'),\n",
       " ('asia', 'NN'),\n",
       " ('know', 'VBP'),\n",
       " ('about', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('.', '.'),\n",
       " ('this', 'DT'),\n",
       " ('makes', 'VBZ'),\n",
       " ('beijing', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('decision', 'NN'),\n",
       " ('all', 'DT'),\n",
       " ('too', 'RB'),\n",
       " ('predictable', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('your', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('version', 'NN'),\n",
       " ('read', 'VBD'),\n",
       " ('this', 'DT'),\n",
       " (':', ':'),\n",
       " ('.', '.'),\n",
       " ('tl', 'NN'),\n",
       " (';', ':'),\n",
       " ('dr', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('briefing', 'VBG'),\n",
       " ('|', 'JJ'),\n",
       " ('asia', 'NN'),\n",
       " (':', ':'),\n",
       " ('china', 'NN'),\n",
       " (':', ':'),\n",
       " ('officials', 'NNS'),\n",
       " ('say', 'VBP'),\n",
       " ('virus', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('spreading', 'VBG'),\n",
       " ('fast', 'RB'),\n",
       " ('over', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('territory', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('word', 'NN'),\n",
       " ('spreads', 'VBZ'),\n",
       " ('across', 'IN'),\n",
       " ('china', 'NN'),\n",
       " (',', ','),\n",
       " ('authorities', 'NNS'),\n",
       " ('struggle', 'VBP'),\n",
       " ('against', 'IN'),\n",
       " ('possible', 'JJ'),\n",
       " ('pandemic', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " (',', ','),\n",
       " ('scientists', 'NNS'),\n",
       " ('report', 'VBP'),\n",
       " ('finding', 'VBG'),\n",
       " ('signs', 'NNS'),\n",
       " ('pointing', 'VBG'),\n",
       " ('toward', 'IN'),\n",
       " ('emergence', 'NN'),\n",
       " ('next', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('among', 'IN'),\n",
       " ('pigs', 'NNS'),\n",
       " ('carrying', 'VBG'),\n",
       " ('highly', 'RB'),\n",
       " ('contagious', 'JJ'),\n",
       " ('virus', 'NN'),\n",
       " (',', ','),\n",
       " ('causing', 'VBG'),\n",
       " ('disease', 'NN'),\n",
       " ('known', 'VBN'),\n",
       " ('locally', 'RB'),\n",
       " ('only', 'RB'),\n",
       " ('recently', 'RB'),\n",
       " ('found', 'VBN'),\n",
       " ('near', 'IN'),\n",
       " ('shanghai', 'NN'),\n",
       " ('zoo', 'NN'),\n",
       " ('where', 'WRB'),\n",
       " ('three', 'CD'),\n",
       " ('weeks', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " (';', ':'),\n",
       " ('photo', 'NN'),\n",
       " ('(', '('),\n",
       " ('m', 'NN'),\n",
       " (')', ')'),\n",
       " ('(', '('),\n",
       " ('careas', 'NNS'),\n",
       " (')', ')'),\n",
       " ('(', '('),\n",
       " ('s', 'NN'),\n",
       " (')', ')'),\n",
       " ('(', '('),\n",
       " ('front', 'JJ'),\n",
       " ('page', 'NN'),\n",
       " ('dept', 'JJ'),\n",
       " ('column', 'NN'),\n",
       " (')', ')'),\n",
       " ('because', 'IN'),\n",
       " ('government', 'NN'),\n",
       " ('says', 'VBZ'),\n",
       " ('public', 'JJ'),\n",
       " ('reaction', 'NN'),\n",
       " ('seems', 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " ('saying', 'VBG'),\n",
       " ('government', 'NN'),\n",
       " ('response', 'NN'),\n",
       " ('appears', 'VBZ'),\n",
       " ('slow', 'JJ'),\n",
       " ('(', '('),\n",
       " ('l', 'NN'),\n",
       " (')', ')'),\n",
       " ('(', '('),\n",
       " ('s', 'NN'),\n",
       " (')', ')'),\n",
       " ('/', 'VBZ'),\n",
       " ('no', 'DT'),\n",
       " ('matter', 'NN'),\n",
       " ('how', 'WRB'),\n",
       " ('can', 'MD'),\n",
       " ('help', 'VB'),\n",
       " ('stop', 'VB'),\n",
       " ('spreading', 'VBG'),\n",
       " ('quickly', 'RB'),\n",
       " ('enough', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('frame', 'VB'),\n",
       " ('up', 'RP'),\n",
       " ('ahead', 'RB'),\n",
       " ('(', '('),\n",
       " ('news', 'NN'),\n",
       " ('media', 'NNS'),\n",
       " ('coverage', 'NN'),\n",
       " (';', ':'),\n",
       " ('photos', 'NNS'),\n",
       " ('(', '('),\n",
       " ('mar', 'NN'),\n",
       " ('pretty', 'RB'),\n",
       " ('fast', 'RB'),\n",
       " ('enough', 'RB'),\n",
       " (')', ')'),\n",
       " ('being', 'VBG'),\n",
       " ('reported', 'VBN'),\n",
       " ('yesterday', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('us', 'PRP'),\n",
       " ('govt', 'VBP'),\n",
       " ('ts', 'JJ'),\n",
       " ('de', 'FW'),\n",
       " ('la', 'FW'),\n",
       " ('y', 'FW'),\n",
       " ('reuters', 'NNS'),\n",
       " ('(', '('),\n",
       " ('nyt', 'NN'),\n",
       " (')', ')'),\n",
       " (':', ':'),\n",
       " ('bloomberg', 'NN'),\n",
       " ('reports', 'NNS'),\n",
       " ('swine', 'NN'),\n",
       " ('flu', 'NN'),\n",
       " ('crisis', 'NN'),\n",
       " ('shows', 'VBZ'),\n",
       " ('two', 'CD'),\n",
       " ('days', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('.', '.'),\n",
       " ('photo', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('elaine', 'JJ'),\n",
       " ('l.', 'NN'),\n",
       " ('allen', 'JJ'),\n",
       " ('st.', 'NN'),\n",
       " ('louis', 'NN'),\n",
       " ('times', 'NNS'),\n",
       " ('london', 'VBP'),\n",
       " ('(', '('),\n",
       " ('ap', 'NN'),\n",
       " (')', ')'),\n",
       " ('-', ':'),\n",
       " ('nyt', 'NN'),\n",
       " (':', ':'),\n",
       " ('new', 'JJ'),\n",
       " ('york', 'NN'),\n",
       " ('post', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('top', 'JJ'),\n",
       " ('city', 'NN'),\n",
       " ('official', 'NN'),\n",
       " ('warns', 'NNS'),\n",
       " ('american', 'JJ'),\n",
       " ('express', 'JJ'),\n",
       " ('newspaper', 'NN'),\n",
       " ('le', 'NN'),\n",
       " ('monde', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('article', 'NN'),\n",
       " ('reporting', 'VBG'),\n",
       " ('wednesday', 'NN'),\n",
       " ('may', 'MD'),\n",
       " ('16', 'CD'),\n",
       " ('march', 'NN'),\n",
       " ('16', 'CD'),\n",
       " (',', ','),\n",
       " ('2008', 'CD'),\n",
       " ('feb', 'NN'),\n",
       " ('17', 'CD'),\n",
       " (',', ','),\n",
       " ('2002', 'CD'),\n",
       " ('united', 'JJ'),\n",
       " ('states', 'NNS'),\n",
       " ('february', 'JJ'),\n",
       " ('18', 'CD'),\n",
       " (',', ','),\n",
       " ('2003', 'CD'),\n",
       " ('june', 'NN'),\n",
       " ('25', 'CD'),\n",
       " (',', ','),\n",
       " ('2007', 'CD'),\n",
       " (',', ','),\n",
       " ('france', 'NN'),\n",
       " ('18', 'CD'),\n",
       " ('april', 'JJ'),\n",
       " ('11', 'CD'),\n",
       " (',', ','),\n",
       " ('2006', 'CD'),\n",
       " ('national', 'JJ'),\n",
       " ('security', 'NN'),\n",
       " ('council', 'NN'),\n",
       " ('release', 'NN'),\n",
       " ('du', 'VBZ'),\n",
       " ('23', 'CD'),\n",
       " ('january', 'JJ'),\n",
       " ('22', 'CD'),\n",
       " (',', ','),\n",
       " ('2005', 'CD'),\n",
       " ('los', 'NN'),\n",
       " ('angeles', 'NNS'),\n",
       " ('et', 'VBP'),\n",
       " ('al', 'JJ'),\n",
       " ('qaeda', 'NN'),\n",
       " ('17', 'CD'),\n",
       " ('september', 'NN'),\n",
       " ('23', 'CD'),\n",
       " (',', ','),\n",
       " ('2009', 'CD'),\n",
       " ('sunday', 'NN'),\n",
       " ('22', 'CD'),\n",
       " ('le', 'NN'),\n",
       " ('20', 'CD'),\n",
       " ('november', 'RB'),\n",
       " ('26', 'CD'),\n",
       " ('h', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('cest', 'JJS'),\n",
       " ('un', 'JJ'),\n",
       " ('el', 'NN'),\n",
       " ('19', 'CD'),\n",
       " ('december', 'NN'),\n",
       " ('12', 'CD'),\n",
       " ('mar', 'NN'),\n",
       " ('2004', 'CD'),\n",
       " ('au', 'NN'),\n",
       " ('24', 'CD'),\n",
       " ('2001', 'CD'),\n",
       " ('*', 'NN'),\n",
       " ('2', 'CD'),\n",
       " ('la', 'NN'),\n",
       " ('primera', 'NN'),\n",
       " ('ber', 'NN'),\n",
       " ('c', 'NN'),\n",
       " ('en', 'IN'),\n",
       " ('lundi', 'JJ'),\n",
       " ('g', 'NN'),\n",
       " ('uire', 'NN'),\n",
       " ('sd', 'NN'),\n",
       " ('paris', 'NN'),\n",
       " ('anl', 'JJ'),\n",
       " ('nterme', 'JJ'),\n",
       " (\"d'aillez\", 'NN'),\n",
       " ('bre', 'NN'),\n",
       " ('ment', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = []\n",
    "for i in pos:\n",
    "    new_pos.append([item for sublist in i for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9594"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pos(data, label):\n",
    "            \n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear', gamma=\"auto\")  \n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    train_corpus = [\" \".join(x) for x in train_corpus]\n",
    "\n",
    "    test_corpus = X_test\n",
    "    test_corpus = [\" \".join(x) for x in test_corpus]\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    print(classification_report(y_test, pred, labels = ['ctrl', 'gpt', 'gpt2', 'grover','xlm', 'xlnet', 'pplm', 'human', 'fair'],\n",
    "                                digits=4))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl     0.4708    0.6056    0.5298       213\n",
      "         gpt     0.6476    0.6385    0.6430       213\n",
      "        gpt2     0.5823    0.6776    0.6263       214\n",
      "      grover     0.2892    0.3897    0.3320       213\n",
      "         xlm     0.7568    0.6573    0.7035       213\n",
      "       xlnet     0.6371    0.7089    0.6711       213\n",
      "        pplm     0.3737    0.3458    0.3592       214\n",
      "       human     0.3203    0.1925    0.2405       213\n",
      "        fair     0.6821    0.4836    0.5659       213\n",
      "\n",
      "    accuracy                         0.5221      1919\n",
      "   macro avg     0.5289    0.5222    0.5190      1919\n",
      "weighted avg     0.5288    0.5221    0.5190      1919\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6405     pplm\n",
       " 2343     gpt2\n",
       " 8741     fair\n",
       " 2295     gpt2\n",
       " 8320    human\n",
       "         ...  \n",
       " 5976    xlnet\n",
       " 4769      xlm\n",
       " 8228    human\n",
       " 4864      xlm\n",
       " 8476    human\n",
       " Name: class, Length: 1919, dtype: object,\n",
       " array(['pplm', 'gpt2', 'human', ..., 'human', 'xlm', 'grover'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_pos(check_pos, data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Adaku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pos = []\n",
    "for i in pos:\n",
    "    words = nltk.FreqDist(tag for (word, tag) in i)\n",
    "    check_pos.append(list(words.keys()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9594"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN',\n",
       " 'VBZ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'POS',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'WDT',\n",
       " 'VBN',\n",
       " 'JJR',\n",
       " 'CD',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " ',',\n",
       " '.',\n",
       " 'CC',\n",
       " 'PRP',\n",
       " 'VBG',\n",
       " 'PRP$',\n",
       " ':',\n",
       " 'WP',\n",
       " 'EX',\n",
       " 'MD',\n",
       " 'WRB',\n",
       " 'VBD',\n",
       " \"''\",\n",
       " 'JJS',\n",
       " 'RP',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'FW',\n",
       " 'NNP']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [\" \".join(x) for x in check_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9594"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame({'POS': pp, 'class': data['class']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.to_csv('pos.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    " \n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    \n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = []\n",
    "for i in data['text']:\n",
    "    n_grams.append(extract_ngrams(i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_grams = []\n",
    "for i in n_grams:\n",
    "    new_n_grams.append([item for sublist in i for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl     1.0000    1.0000    1.0000       213\n",
      "         gpt     1.0000    1.0000    1.0000       213\n",
      "        gpt2     0.6774    0.6869    0.6821       214\n",
      "      grover     0.6173    0.7042    0.6579       213\n",
      "         xlm     1.0000    1.0000    1.0000       213\n",
      "       xlnet     0.9953    0.9906    0.9929       213\n",
      "        pplm     0.9133    0.7383    0.8165       214\n",
      "       human     0.7727    0.6385    0.6992       213\n",
      "        fair     0.5907    0.7183    0.6483       213\n",
      "\n",
      "    accuracy                         0.8306      1919\n",
      "   macro avg     0.8407    0.8308    0.8330      1919\n",
      "weighted avg     0.8407    0.8306    0.8329      1919\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6405     pplm\n",
       " 2343     gpt2\n",
       " 8741     fair\n",
       " 2295     gpt2\n",
       " 8320    human\n",
       "         ...  \n",
       " 5976    xlnet\n",
       " 4769      xlm\n",
       " 8228    human\n",
       " 4864      xlm\n",
       " 8476    human\n",
       " Name: class, Length: 1919, dtype: object,\n",
       " array(['pplm', 'gpt2', 'fair', ..., 'human', 'xlm', 'human'], dtype=object))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_pos(n_grams, data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china wants to',\n",
       " 'wants to take',\n",
       " 'to take a',\n",
       " 'take a victory',\n",
       " 'a victory lap',\n",
       " 'victory lap over',\n",
       " 'lap over its',\n",
       " 'over its handling',\n",
       " 'its handling of',\n",
       " 'handling of the',\n",
       " 'of the coronavirus',\n",
       " 'the coronavirus outbreak',\n",
       " 'coronavirus outbreak in',\n",
       " 'outbreak in hong',\n",
       " 'in hong kong',\n",
       " 'hong kong .',\n",
       " 'kong . the',\n",
       " '. the virus',\n",
       " 'the virus has',\n",
       " 'virus has killed',\n",
       " 'has killed at',\n",
       " 'killed at least',\n",
       " 'at least 21',\n",
       " 'least 21 people',\n",
       " '21 people and',\n",
       " 'people and infected',\n",
       " 'and infected more',\n",
       " 'infected more than',\n",
       " 'more than 1,000',\n",
       " 'than 1,000 since',\n",
       " '1,000 since it',\n",
       " 'since it was',\n",
       " 'it was first',\n",
       " 'was first detected',\n",
       " 'first detected there',\n",
       " 'detected there last',\n",
       " 'there last month',\n",
       " 'last month .',\n",
       " 'month . it',\n",
       " '. it is',\n",
       " 'it is believed',\n",
       " 'is believed that',\n",
       " 'believed that most',\n",
       " 'that most have',\n",
       " 'most have been',\n",
       " 'have been chinese',\n",
       " 'been chinese citizens',\n",
       " 'chinese citizens who',\n",
       " 'citizens who had',\n",
       " 'who had traveled',\n",
       " 'had traveled abroad',\n",
       " 'traveled abroad for',\n",
       " 'abroad for treatment',\n",
       " 'for treatment or',\n",
       " 'treatment or were',\n",
       " 'or were working',\n",
       " 'were working as',\n",
       " 'working as doctors',\n",
       " 'as doctors there',\n",
       " 'doctors there .',\n",
       " 'there . but',\n",
       " '. but health',\n",
       " 'but health officials',\n",
       " 'health officials say',\n",
       " 'officials say they',\n",
       " 'say they are',\n",
       " 'they are still',\n",
       " 'are still investigating',\n",
       " 'still investigating whether',\n",
       " 'investigating whether anyone',\n",
       " 'whether anyone else',\n",
       " 'anyone else may',\n",
       " 'else may be',\n",
       " 'may be infected',\n",
       " 'be infected .',\n",
       " 'infected . a',\n",
       " '. a total',\n",
       " 'a total ban',\n",
       " 'total ban on',\n",
       " 'ban on travel',\n",
       " 'on travel from',\n",
       " 'travel from this',\n",
       " 'from this country',\n",
       " 'this country remains',\n",
       " 'country remains until',\n",
       " 'remains until further',\n",
       " 'until further notice',\n",
       " 'further notice .',\n",
       " 'notice . [',\n",
       " '. [ page',\n",
       " '[ page a8',\n",
       " 'page a8 .',\n",
       " 'a8 . ]',\n",
       " '. ] in',\n",
       " '] in addition',\n",
       " 'in addition today',\n",
       " 'addition today ,',\n",
       " 'today , china',\n",
       " \", china 's\",\n",
       " \"china 's ministry\",\n",
       " \"'s ministry of\",\n",
       " 'ministry of health',\n",
       " 'of health said',\n",
       " 'health said tests',\n",
       " 'said tests showed',\n",
       " 'tests showed evidence',\n",
       " 'showed evidence suggesting',\n",
       " 'evidence suggesting an',\n",
       " 'suggesting an unusual',\n",
       " 'an unusual strain',\n",
       " 'unusual strain called',\n",
       " 'strain called sars',\n",
       " 'called sars could',\n",
       " 'sars could spread',\n",
       " 'could spread through',\n",
       " 'spread through humans',\n",
       " 'through humans if',\n",
       " 'humans if not',\n",
       " 'if not properly',\n",
       " 'not properly treated',\n",
       " 'properly treated .',\n",
       " 'treated . [',\n",
       " '. [ excerpts',\n",
       " '[ excerpts ,',\n",
       " 'excerpts , page',\n",
       " ', page b6',\n",
       " 'page b6 .',\n",
       " 'b6 . ]',\n",
       " '. ] dr.',\n",
       " '] dr. wu',\n",
       " 'dr. wu jianmin',\n",
       " 'wu jianmin ,',\n",
       " 'jianmin , director',\n",
       " ', director general',\n",
       " 'director general with',\n",
       " 'general with guangdong',\n",
       " 'with guangdong province',\n",
       " 'guangdong province ,',\n",
       " 'province , which',\n",
       " ', which includes',\n",
       " 'which includes guangzhou',\n",
       " 'includes guangzhou ,',\n",
       " 'guangzhou , told',\n",
       " ', told reporters',\n",
       " 'told reporters today',\n",
       " 'reporters today he',\n",
       " 'today he expected',\n",
       " 'he expected a',\n",
       " 'expected a significant',\n",
       " 'a significant outbreak',\n",
       " 'significant outbreak within',\n",
       " 'outbreak within days',\n",
       " 'within days .',\n",
       " 'days . he',\n",
       " '. he added',\n",
       " 'he added later',\n",
       " 'added later by',\n",
       " 'later by telephone',\n",
       " 'by telephone that',\n",
       " 'telephone that ,',\n",
       " 'that , we',\n",
       " \", we 're\",\n",
       " \"we 're prepared\",\n",
       " \"'re prepared should\",\n",
       " 'prepared should something',\n",
       " 'should something happen',\n",
       " 'something happen .',\n",
       " 'happen . at',\n",
       " '. at issue',\n",
       " 'at issue here',\n",
       " 'issue here will',\n",
       " 'here will likely',\n",
       " 'will likely turn',\n",
       " 'likely turn out',\n",
       " 'turn out either',\n",
       " 'out either one',\n",
       " 'either one way',\n",
       " 'one way --',\n",
       " 'way -- another',\n",
       " '-- another case',\n",
       " 'another case confirms',\n",
       " 'case confirms what',\n",
       " 'confirms what many',\n",
       " 'what many suspect',\n",
       " 'many suspect but',\n",
       " 'suspect but no',\n",
       " 'but no conclusive',\n",
       " 'no conclusive proof',\n",
       " 'conclusive proof supports',\n",
       " 'proof supports it',\n",
       " 'supports it .',\n",
       " 'it . either',\n",
       " '. either way',\n",
       " 'either way ,',\n",
       " 'way , beijing',\n",
       " ', beijing would',\n",
       " 'beijing would probably',\n",
       " 'would probably try',\n",
       " 'probably try hard',\n",
       " 'try hard just',\n",
       " 'hard just now',\n",
       " 'just now before',\n",
       " 'now before rushing',\n",
       " 'before rushing into',\n",
       " 'rushing into any',\n",
       " 'into any new',\n",
       " 'any new action',\n",
       " 'new action .',\n",
       " 'action . and',\n",
       " '. and while',\n",
       " 'and while some',\n",
       " 'while some experts',\n",
       " 'some experts believe',\n",
       " 'experts believe such',\n",
       " 'believe such action',\n",
       " 'such action might',\n",
       " 'action might delay',\n",
       " 'might delay even',\n",
       " 'delay even worse',\n",
       " 'even worse problems',\n",
       " 'worse problems ,',\n",
       " 'problems , others',\n",
       " ', others think',\n",
       " 'others think it',\n",
       " \"think it 's\",\n",
       " \"it 's better\",\n",
       " \"'s better late\",\n",
       " 'better late rather',\n",
       " 'late rather then',\n",
       " 'rather then never',\n",
       " 'then never .',\n",
       " 'never . even',\n",
       " '. even so',\n",
       " 'even so ,',\n",
       " 'so , both',\n",
       " ', both sides',\n",
       " 'both sides seem',\n",
       " 'sides seem intent',\n",
       " 'seem intent upon',\n",
       " 'intent upon taking',\n",
       " 'upon taking advantage',\n",
       " 'taking advantage now',\n",
       " 'advantage now .',\n",
       " 'now . with',\n",
       " '. with so',\n",
       " 'with so much',\n",
       " 'so much attention',\n",
       " 'much attention focused',\n",
       " 'attention focused elsewhere',\n",
       " 'focused elsewhere ,',\n",
       " 'elsewhere , few',\n",
       " ', few outside',\n",
       " 'few outside asia',\n",
       " 'outside asia know',\n",
       " 'asia know about',\n",
       " 'know about this',\n",
       " 'about this .',\n",
       " 'this . this',\n",
       " '. this makes',\n",
       " 'this makes beijing',\n",
       " \"makes beijing 's\",\n",
       " \"beijing 's decision\",\n",
       " \"'s decision all\",\n",
       " 'decision all too',\n",
       " 'all too predictable',\n",
       " 'too predictable .',\n",
       " 'predictable . if',\n",
       " '. if you',\n",
       " 'if you want',\n",
       " 'you want your',\n",
       " 'want your own',\n",
       " 'your own version',\n",
       " 'own version read',\n",
       " 'version read this',\n",
       " 'read this :',\n",
       " 'this : .',\n",
       " ': . tl',\n",
       " '. tl ;',\n",
       " 'tl ; dr',\n",
       " '; dr world',\n",
       " 'dr world briefing',\n",
       " 'world briefing |',\n",
       " 'briefing | asia',\n",
       " '| asia :',\n",
       " 'asia : china',\n",
       " ': china :',\n",
       " 'china : officials',\n",
       " ': officials say',\n",
       " 'officials say virus',\n",
       " 'say virus is',\n",
       " 'virus is spreading',\n",
       " 'is spreading fast',\n",
       " 'spreading fast over',\n",
       " 'fast over its',\n",
       " 'over its own',\n",
       " 'its own territory',\n",
       " 'own territory as',\n",
       " 'territory as word',\n",
       " 'as word spreads',\n",
       " 'word spreads across',\n",
       " 'spreads across china',\n",
       " 'across china ,',\n",
       " 'china , authorities',\n",
       " ', authorities struggle',\n",
       " 'authorities struggle against',\n",
       " 'struggle against possible',\n",
       " 'against possible pandemic',\n",
       " 'possible pandemic threat',\n",
       " 'pandemic threat ,',\n",
       " 'threat , scientists',\n",
       " ', scientists report',\n",
       " 'scientists report finding',\n",
       " 'report finding signs',\n",
       " 'finding signs pointing',\n",
       " 'signs pointing toward',\n",
       " 'pointing toward emergence',\n",
       " 'toward emergence next',\n",
       " 'emergence next week',\n",
       " 'next week after',\n",
       " 'week after years',\n",
       " 'after years ago',\n",
       " 'years ago among',\n",
       " 'ago among pigs',\n",
       " 'among pigs carrying',\n",
       " 'pigs carrying highly',\n",
       " 'carrying highly contagious',\n",
       " 'highly contagious virus',\n",
       " 'contagious virus ,',\n",
       " 'virus , causing',\n",
       " ', causing disease',\n",
       " 'causing disease known',\n",
       " 'disease known locally',\n",
       " 'known locally only',\n",
       " 'locally only recently',\n",
       " 'only recently found',\n",
       " 'recently found near',\n",
       " 'found near shanghai',\n",
       " 'near shanghai zoo',\n",
       " 'shanghai zoo where',\n",
       " 'zoo where three',\n",
       " 'where three weeks',\n",
       " 'three weeks ago',\n",
       " 'weeks ago ;',\n",
       " 'ago ; photo',\n",
       " '; photo (',\n",
       " 'photo ( m',\n",
       " '( m )',\n",
       " 'm ) (',\n",
       " ') ( careas',\n",
       " '( careas )',\n",
       " 'careas ) (',\n",
       " ') ( s',\n",
       " '( s )',\n",
       " 's ) (',\n",
       " ') ( front',\n",
       " '( front page',\n",
       " 'front page dept',\n",
       " 'page dept column',\n",
       " 'dept column )',\n",
       " 'column ) because',\n",
       " ') because government',\n",
       " 'because government says',\n",
       " 'government says public',\n",
       " 'says public reaction',\n",
       " 'public reaction seems',\n",
       " 'reaction seems like',\n",
       " 'seems like saying',\n",
       " 'like saying government',\n",
       " 'saying government response',\n",
       " 'government response appears',\n",
       " 'response appears slow',\n",
       " 'appears slow (',\n",
       " 'slow ( l',\n",
       " '( l )',\n",
       " 'l ) (',\n",
       " ') ( s',\n",
       " '( s )',\n",
       " 's ) /',\n",
       " ') / no',\n",
       " '/ no matter',\n",
       " 'no matter how',\n",
       " 'matter how can',\n",
       " 'how can help',\n",
       " 'can help stop',\n",
       " 'help stop spreading',\n",
       " 'stop spreading quickly',\n",
       " 'spreading quickly enough',\n",
       " 'quickly enough time',\n",
       " 'enough time frame',\n",
       " 'time frame up',\n",
       " 'frame up ahead',\n",
       " 'up ahead (',\n",
       " 'ahead ( news',\n",
       " '( news media',\n",
       " 'news media coverage',\n",
       " 'media coverage ;',\n",
       " 'coverage ; photos',\n",
       " '; photos (',\n",
       " 'photos ( mar',\n",
       " '( mar pretty',\n",
       " 'mar pretty fast',\n",
       " 'pretty fast enough',\n",
       " 'fast enough )',\n",
       " 'enough ) being',\n",
       " ') being reported',\n",
       " 'being reported yesterday',\n",
       " 'reported yesterday morning',\n",
       " 'yesterday morning us',\n",
       " 'morning us govt',\n",
       " 'us govt ts',\n",
       " 'govt ts de',\n",
       " 'ts de la',\n",
       " 'de la y',\n",
       " 'la y reuters',\n",
       " 'y reuters (',\n",
       " 'reuters ( nyt',\n",
       " '( nyt )',\n",
       " 'nyt ) :',\n",
       " ') : bloomberg',\n",
       " ': bloomberg reports',\n",
       " 'bloomberg reports swine',\n",
       " 'reports swine flu',\n",
       " 'swine flu crisis',\n",
       " 'flu crisis shows',\n",
       " 'crisis shows two',\n",
       " 'shows two days',\n",
       " 'two days ago',\n",
       " 'days ago .',\n",
       " 'ago . photo',\n",
       " '. photo by',\n",
       " 'photo by elaine',\n",
       " 'by elaine l.',\n",
       " 'elaine l. allen',\n",
       " 'l. allen st.',\n",
       " 'allen st. louis',\n",
       " 'st. louis times',\n",
       " 'louis times london',\n",
       " 'times london (',\n",
       " 'london ( ap',\n",
       " '( ap )',\n",
       " 'ap ) -',\n",
       " ') - nyt',\n",
       " '- nyt :',\n",
       " 'nyt : new',\n",
       " ': new york',\n",
       " 'new york post',\n",
       " \"york post 's\",\n",
       " \"post 's top\",\n",
       " \"'s top city\",\n",
       " 'top city official',\n",
       " 'city official warns',\n",
       " 'official warns american',\n",
       " 'warns american express',\n",
       " 'american express newspaper',\n",
       " 'express newspaper le',\n",
       " 'newspaper le monde',\n",
       " 'le monde today',\n",
       " 'monde today article',\n",
       " 'today article reporting',\n",
       " 'article reporting wednesday',\n",
       " 'reporting wednesday may',\n",
       " 'wednesday may 16',\n",
       " 'may 16 march',\n",
       " '16 march 16',\n",
       " 'march 16 ,',\n",
       " '16 , 2008',\n",
       " ', 2008 feb',\n",
       " '2008 feb 17',\n",
       " 'feb 17 ,',\n",
       " '17 , 2002',\n",
       " ', 2002 united',\n",
       " '2002 united states',\n",
       " 'united states february',\n",
       " 'states february 18',\n",
       " 'february 18 ,',\n",
       " '18 , 2003',\n",
       " ', 2003 june',\n",
       " '2003 june 25',\n",
       " 'june 25 ,',\n",
       " '25 , 2007',\n",
       " ', 2007 ,',\n",
       " '2007 , france',\n",
       " ', france 18',\n",
       " 'france 18 april',\n",
       " '18 april 11',\n",
       " 'april 11 ,',\n",
       " '11 , 2006',\n",
       " ', 2006 national',\n",
       " '2006 national security',\n",
       " 'national security council',\n",
       " 'security council release',\n",
       " 'council release du',\n",
       " 'release du 23',\n",
       " 'du 23 january',\n",
       " '23 january 22',\n",
       " 'january 22 ,',\n",
       " '22 , 2005',\n",
       " ', 2005 los',\n",
       " '2005 los angeles',\n",
       " 'los angeles et',\n",
       " 'angeles et al',\n",
       " 'et al qaeda',\n",
       " 'al qaeda 17',\n",
       " 'qaeda 17 september',\n",
       " '17 september 23',\n",
       " 'september 23 ,',\n",
       " '23 , 2009',\n",
       " ', 2009 sunday',\n",
       " '2009 sunday 22',\n",
       " 'sunday 22 le',\n",
       " '22 le 20',\n",
       " 'le 20 november',\n",
       " '20 november 26',\n",
       " 'november 26 h',\n",
       " '26 h e',\n",
       " 'h e cest',\n",
       " 'e cest un',\n",
       " 'cest un el',\n",
       " 'un el 19',\n",
       " 'el 19 december',\n",
       " '19 december 12',\n",
       " 'december 12 mar',\n",
       " '12 mar 2004',\n",
       " 'mar 2004 au',\n",
       " '2004 au 24',\n",
       " 'au 24 2001',\n",
       " '24 2001 *',\n",
       " '2001 * 2',\n",
       " '* 2 la',\n",
       " '2 la primera',\n",
       " 'la primera ber',\n",
       " 'primera ber c',\n",
       " 'ber c en',\n",
       " 'c en lundi',\n",
       " 'en lundi g',\n",
       " 'lundi g uire',\n",
       " 'g uire sd',\n",
       " 'uire sd paris',\n",
       " 'sd paris anl',\n",
       " 'paris anl nterme',\n",
       " \"anl nterme d'aillez\",\n",
       " \"nterme d'aillez bre\",\n",
       " \"d'aillez bre ment\"]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this',)\n",
      "('is',)\n",
      "('a',)\n",
      "('foo',)\n",
      "('bar',)\n",
      "('sentences',)\n",
      "('and',)\n",
      "('i',)\n",
      "('want',)\n",
      "('to',)\n",
      "('ngramize',)\n",
      "('it',)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "def gram(sentence, n):\n",
    "    \n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x0000029A2A142B10>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_grams = []\n",
    "for i in data['text']:\n",
    "    character_grams.append(extract_ngrams(i, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'victory',\n",
       " 'lap',\n",
       " 'over',\n",
       " 'its',\n",
       " 'handling',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coronavirus',\n",
       " 'outbreak',\n",
       " 'in',\n",
       " 'hong',\n",
       " 'kong',\n",
       " '.',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'has',\n",
       " 'killed',\n",
       " 'at',\n",
       " 'least',\n",
       " '21',\n",
       " 'people',\n",
       " 'and',\n",
       " 'infected',\n",
       " 'more',\n",
       " 'than',\n",
       " '1,000',\n",
       " 'since',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'detected',\n",
       " 'there',\n",
       " 'last',\n",
       " 'month',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'most',\n",
       " 'have',\n",
       " 'been',\n",
       " 'chinese',\n",
       " 'citizens',\n",
       " 'who',\n",
       " 'had',\n",
       " 'traveled',\n",
       " 'abroad',\n",
       " 'for',\n",
       " 'treatment',\n",
       " 'or',\n",
       " 'were',\n",
       " 'working',\n",
       " 'as',\n",
       " 'doctors',\n",
       " 'there',\n",
       " '.',\n",
       " 'but',\n",
       " 'health',\n",
       " 'officials',\n",
       " 'say',\n",
       " 'they',\n",
       " 'are',\n",
       " 'still',\n",
       " 'investigating',\n",
       " 'whether',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'may',\n",
       " 'be',\n",
       " 'infected',\n",
       " '.',\n",
       " 'a',\n",
       " 'total',\n",
       " 'ban',\n",
       " 'on',\n",
       " 'travel',\n",
       " 'from',\n",
       " 'this',\n",
       " 'country',\n",
       " 'remains',\n",
       " 'until',\n",
       " 'further',\n",
       " 'notice',\n",
       " '.',\n",
       " '[',\n",
       " 'page',\n",
       " 'a8',\n",
       " '.',\n",
       " ']',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'today',\n",
       " ',',\n",
       " 'china',\n",
       " \"'s\",\n",
       " 'ministry',\n",
       " 'of',\n",
       " 'health',\n",
       " 'said',\n",
       " 'tests',\n",
       " 'showed',\n",
       " 'evidence',\n",
       " 'suggesting',\n",
       " 'an',\n",
       " 'unusual',\n",
       " 'strain',\n",
       " 'called',\n",
       " 'sars',\n",
       " 'could',\n",
       " 'spread',\n",
       " 'through',\n",
       " 'humans',\n",
       " 'if',\n",
       " 'not',\n",
       " 'properly',\n",
       " 'treated',\n",
       " '.',\n",
       " '[',\n",
       " 'excerpts',\n",
       " ',',\n",
       " 'page',\n",
       " 'b6',\n",
       " '.',\n",
       " ']',\n",
       " 'dr.',\n",
       " 'wu',\n",
       " 'jianmin',\n",
       " ',',\n",
       " 'director',\n",
       " 'general',\n",
       " 'with',\n",
       " 'guangdong',\n",
       " 'province',\n",
       " ',',\n",
       " 'which',\n",
       " 'includes',\n",
       " 'guangzhou',\n",
       " ',',\n",
       " 'told',\n",
       " 'reporters',\n",
       " 'today',\n",
       " 'he',\n",
       " 'expected',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'outbreak',\n",
       " 'within',\n",
       " 'days',\n",
       " '.',\n",
       " 'he',\n",
       " 'added',\n",
       " 'later',\n",
       " 'by',\n",
       " 'telephone',\n",
       " 'that',\n",
       " ',',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'prepared',\n",
       " 'should',\n",
       " 'something',\n",
       " 'happen',\n",
       " '.',\n",
       " 'at',\n",
       " 'issue',\n",
       " 'here',\n",
       " 'will',\n",
       " 'likely',\n",
       " 'turn',\n",
       " 'out',\n",
       " 'either',\n",
       " 'one',\n",
       " 'way',\n",
       " '--',\n",
       " 'another',\n",
       " 'case',\n",
       " 'confirms',\n",
       " 'what',\n",
       " 'many',\n",
       " 'suspect',\n",
       " 'but',\n",
       " 'no',\n",
       " 'conclusive',\n",
       " 'proof',\n",
       " 'supports',\n",
       " 'it',\n",
       " '.',\n",
       " 'either',\n",
       " 'way',\n",
       " ',',\n",
       " 'beijing',\n",
       " 'would',\n",
       " 'probably',\n",
       " 'try',\n",
       " 'hard',\n",
       " 'just',\n",
       " 'now',\n",
       " 'before',\n",
       " 'rushing',\n",
       " 'into',\n",
       " 'any',\n",
       " 'new',\n",
       " 'action',\n",
       " '.',\n",
       " 'and',\n",
       " 'while',\n",
       " 'some',\n",
       " 'experts',\n",
       " 'believe',\n",
       " 'such',\n",
       " 'action',\n",
       " 'might',\n",
       " 'delay',\n",
       " 'even',\n",
       " 'worse',\n",
       " 'problems',\n",
       " ',',\n",
       " 'others',\n",
       " 'think',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'better',\n",
       " 'late',\n",
       " 'rather',\n",
       " 'then',\n",
       " 'never',\n",
       " '.',\n",
       " 'even',\n",
       " 'so',\n",
       " ',',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'seem',\n",
       " 'intent',\n",
       " 'upon',\n",
       " 'taking',\n",
       " 'advantage',\n",
       " 'now',\n",
       " '.',\n",
       " 'with',\n",
       " 'so',\n",
       " 'much',\n",
       " 'attention',\n",
       " 'focused',\n",
       " 'elsewhere',\n",
       " ',',\n",
       " 'few',\n",
       " 'outside',\n",
       " 'asia',\n",
       " 'know',\n",
       " 'about',\n",
       " 'this',\n",
       " '.',\n",
       " 'this',\n",
       " 'makes',\n",
       " 'beijing',\n",
       " \"'s\",\n",
       " 'decision',\n",
       " 'all',\n",
       " 'too',\n",
       " 'predictable',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'want',\n",
       " 'your',\n",
       " 'own',\n",
       " 'version',\n",
       " 'read',\n",
       " 'this',\n",
       " ':',\n",
       " '.',\n",
       " 'tl',\n",
       " ';',\n",
       " 'dr',\n",
       " 'world',\n",
       " 'briefing',\n",
       " '|',\n",
       " 'asia',\n",
       " ':',\n",
       " 'china',\n",
       " ':',\n",
       " 'officials',\n",
       " 'say',\n",
       " 'virus',\n",
       " 'is',\n",
       " 'spreading',\n",
       " 'fast',\n",
       " 'over',\n",
       " 'its',\n",
       " 'own',\n",
       " 'territory',\n",
       " 'as',\n",
       " 'word',\n",
       " 'spreads',\n",
       " 'across',\n",
       " 'china',\n",
       " ',',\n",
       " 'authorities',\n",
       " 'struggle',\n",
       " 'against',\n",
       " 'possible',\n",
       " 'pandemic',\n",
       " 'threat',\n",
       " ',',\n",
       " 'scientists',\n",
       " 'report',\n",
       " 'finding',\n",
       " 'signs',\n",
       " 'pointing',\n",
       " 'toward',\n",
       " 'emergence',\n",
       " 'next',\n",
       " 'week',\n",
       " 'after',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'among',\n",
       " 'pigs',\n",
       " 'carrying',\n",
       " 'highly',\n",
       " 'contagious',\n",
       " 'virus',\n",
       " ',',\n",
       " 'causing',\n",
       " 'disease',\n",
       " 'known',\n",
       " 'locally',\n",
       " 'only',\n",
       " 'recently',\n",
       " 'found',\n",
       " 'near',\n",
       " 'shanghai',\n",
       " 'zoo',\n",
       " 'where',\n",
       " 'three',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " ';',\n",
       " 'photo',\n",
       " '(',\n",
       " 'm',\n",
       " ')',\n",
       " '(',\n",
       " 'careas',\n",
       " ')',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " '(',\n",
       " 'front',\n",
       " 'page',\n",
       " 'dept',\n",
       " 'column',\n",
       " ')',\n",
       " 'because',\n",
       " 'government',\n",
       " 'says',\n",
       " 'public',\n",
       " 'reaction',\n",
       " 'seems',\n",
       " 'like',\n",
       " 'saying',\n",
       " 'government',\n",
       " 'response',\n",
       " 'appears',\n",
       " 'slow',\n",
       " '(',\n",
       " 'l',\n",
       " ')',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " '/',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'how',\n",
       " 'can',\n",
       " 'help',\n",
       " 'stop',\n",
       " 'spreading',\n",
       " 'quickly',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'frame',\n",
       " 'up',\n",
       " 'ahead',\n",
       " '(',\n",
       " 'news',\n",
       " 'media',\n",
       " 'coverage',\n",
       " ';',\n",
       " 'photos',\n",
       " '(',\n",
       " 'mar',\n",
       " 'pretty',\n",
       " 'fast',\n",
       " 'enough',\n",
       " ')',\n",
       " 'being',\n",
       " 'reported',\n",
       " 'yesterday',\n",
       " 'morning',\n",
       " 'us',\n",
       " 'govt',\n",
       " 'ts',\n",
       " 'de',\n",
       " 'la',\n",
       " 'y',\n",
       " 'reuters',\n",
       " '(',\n",
       " 'nyt',\n",
       " ')',\n",
       " ':',\n",
       " 'bloomberg',\n",
       " 'reports',\n",
       " 'swine',\n",
       " 'flu',\n",
       " 'crisis',\n",
       " 'shows',\n",
       " 'two',\n",
       " 'days',\n",
       " 'ago',\n",
       " '.',\n",
       " 'photo',\n",
       " 'by',\n",
       " 'elaine',\n",
       " 'l.',\n",
       " 'allen',\n",
       " 'st.',\n",
       " 'louis',\n",
       " 'times',\n",
       " 'london',\n",
       " '(',\n",
       " 'ap',\n",
       " ')',\n",
       " '-',\n",
       " 'nyt',\n",
       " ':',\n",
       " 'new',\n",
       " 'york',\n",
       " 'post',\n",
       " \"'s\",\n",
       " 'top',\n",
       " 'city',\n",
       " 'official',\n",
       " 'warns',\n",
       " 'american',\n",
       " 'express',\n",
       " 'newspaper',\n",
       " 'le',\n",
       " 'monde',\n",
       " 'today',\n",
       " 'article',\n",
       " 'reporting',\n",
       " 'wednesday',\n",
       " 'may',\n",
       " '16',\n",
       " 'march',\n",
       " '16',\n",
       " ',',\n",
       " '2008',\n",
       " 'feb',\n",
       " '17',\n",
       " ',',\n",
       " '2002',\n",
       " 'united',\n",
       " 'states',\n",
       " 'february',\n",
       " '18',\n",
       " ',',\n",
       " '2003',\n",
       " 'june',\n",
       " '25',\n",
       " ',',\n",
       " '2007',\n",
       " ',',\n",
       " 'france',\n",
       " '18',\n",
       " 'april',\n",
       " '11',\n",
       " ',',\n",
       " '2006',\n",
       " 'national',\n",
       " 'security',\n",
       " 'council',\n",
       " 'release',\n",
       " 'du',\n",
       " '23',\n",
       " 'january',\n",
       " '22',\n",
       " ',',\n",
       " '2005',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'et',\n",
       " 'al',\n",
       " 'qaeda',\n",
       " '17',\n",
       " 'september',\n",
       " '23',\n",
       " ',',\n",
       " '2009',\n",
       " 'sunday',\n",
       " '22',\n",
       " 'le',\n",
       " '20',\n",
       " 'november',\n",
       " '26',\n",
       " 'h',\n",
       " 'e',\n",
       " 'cest',\n",
       " 'un',\n",
       " 'el',\n",
       " '19',\n",
       " 'december',\n",
       " '12',\n",
       " 'mar',\n",
       " '2004',\n",
       " 'au',\n",
       " '24',\n",
       " '2001',\n",
       " '*',\n",
       " '2',\n",
       " 'la',\n",
       " 'primera',\n",
       " 'ber',\n",
       " 'c',\n",
       " 'en',\n",
       " 'lundi',\n",
       " 'g',\n",
       " 'uire',\n",
       " 'sd',\n",
       " 'paris',\n",
       " 'anl',\n",
       " 'nterme',\n",
       " \"d'aillez\",\n",
       " 'bre',\n",
       " 'ment']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_grams[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ctrl     1.0000    1.0000    1.0000       213\n",
      "         gpt     1.0000    1.0000    1.0000       213\n",
      "        gpt2     0.6712    0.6963    0.6835       214\n",
      "      grover     0.6167    0.6948    0.6534       213\n",
      "         xlm     1.0000    1.0000    1.0000       213\n",
      "       xlnet     0.9953    0.9906    0.9929       213\n",
      "        pplm     0.9075    0.7336    0.8114       214\n",
      "       human     0.7697    0.6432    0.7008       213\n",
      "        fair     0.5882    0.7042    0.6410       213\n",
      "\n",
      "    accuracy                         0.8291      1919\n",
      "   macro avg     0.8387    0.8292    0.8314      1919\n",
      "weighted avg     0.8387    0.8291    0.8314      1919\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6405     pplm\n",
       " 2343     gpt2\n",
       " 8741     fair\n",
       " 2295     gpt2\n",
       " 8320    human\n",
       "         ...  \n",
       " 5976    xlnet\n",
       " 4769      xlm\n",
       " 8228    human\n",
       " 4864      xlm\n",
       " 8476    human\n",
       " Name: class, Length: 1919, dtype: object,\n",
       " array(['pplm', 'gpt2', 'fair', ..., 'human', 'xlm', 'human'], dtype=object))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_pos(character_grams, data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
